---
title: 'મલ્ટિ-પેજ ક્રોલિંગ'
publishedAt: '2025-03-25'
summary: 'પરસ્પર જોડાયેલી વેબસાઇટ્સને ક્રોલ કરવા, સાઇટમેપ્સ મેનેજ કરવા અને ડુપ્લિકેટ કન્ટેન્ટ હેન્ડલ કરવાની તકનીકોમાં માસ્ટરી મેળવો'
---

સ્ટેટિક અને ડાયનેમિક કન્ટેન્ટ સ્ક્રેપિંગ બંનેના મૂળભૂત સિદ્ધાંતો સમજ્યા પછી, હવે વધુ વ્યાપક પડકારનો સામનો કરવાનો સમય છે: મલ્ટિ-પેજ ક્રોલિંગ. આ વિભાગ બહુવિધ પરસ્પર જોડાયેલા પાનાઓ સાથેની વેબસાઇટ્સથી અસરકારક રીતે નેવિગેટ કરવા અને ડેટા એક્સ્ટ્રેક્ટ કરવા પર ધ્યાન કેન્દ્રિત કરે છે.

મલ્ટિ-પેજ વેબસાઇટ્સ ક્રોલ કરવા માટે બે મુખ્ય અભિગમો છે:

1. લિંક-આધારિત ક્રોલિંગ - પેજો વચ્ચેના લિંક્સને અનુસરવું
2. સાઇટમેપ-આધારિત ક્રોલિંગ - sitemap.xml ફાઇલનો ઉપયોગ કરવો

સાઇટમેપ ક્રોલિંગ માટે, મોટાભાગની વેબસાઇટ્સ sitemap.xml ફાઇલ પ્રદાન કરે છે જે બધા મહત્વપૂર્ણ URLs ની યાદી આપે છે. આ સંરચિત XML ફાઇલમાં શામેલ છે:

- પેજ URLs
- છેલ્લે બદલાયેલી તારીખો
- બદલાવની આવૃત્તિ
- પ્રાથમિકતા મૂલ્યો

સાઇટમેપનો ઉપયોગ લિંક ક્રોલિંગ કરતાં વધુ કાર્યક્ષમ હોઈ શકે છે કારણ કે તે:
- આગોતરું પાનાઓની સંપૂર્ણ યાદી પ્રદાન કરે છે
- પેજની મહત્તા અને તાજગી વિશે મેટાડેટા શામેલ કરે છે
- બિનજરૂરી પાનાઓ ક્રોલ કરવાનું ટાળે છે
- સર્વરનો લોડ ઘટાડે છે

પરંતુ આ પ્રકરણ માટે, આપણે મલ્ટિ-પેજ ઈ-કોમર્સ સાઇટ માટે ક્રોલર બનાવવા માટે Crawlee નો ઉપયોગ કરીને લિંક-આધારિત ક્રોલિંગ પર ધ્યાન કેન્દ્રિત કરીશું. Crawlee વેબ ક્રોલિંગની ઘણી જટિલતાઓનો આપણા માટે ઉકેલ લાવે છે, જેમાં શામેલ છે:

- આપોઆપ કતાર વ્યવસ્થાપન અને URL ડિડ્યુપ્લિકેશન
- બિલ્ટ-ઇન રેટ લિમિટિંગ અને રિટ્રાઇ લોજિક
- કૉન્ફિગરેબલ રિક્વેસ્ટ હેન્ડલિંગ અને રાઉટિંગ
- ડેટા સ્ટોરેજ અને એક્સપોર્ટ

આપણે જે સાઇટ સ્ટ્રક્ચર ક્રોલ કરીશું તે આવું દેખાય છે:

```
હોમપેજ
├── કેટેગરી: ઇલેક્ટ્રોનિક્સ
│   ├── ફોન્સ
│   ├── લેપટોપ્સ
│   └── એક્સેસરીઝ
├── કેટેગરી: વસ્ત્રો
│   ├── પુરુષોનાં
│   └── મહિલાઓનાં
└── ફીચર્ડ પ્રોડક્ટ્સ
```

દરેક પ્રોડક્ટ પેજમાં કેટેગરીના આધારે વિવિધ લેઆઉટ છે, પરંતુ આપણને સુસંગત માહિતી એક્સ્ટ્રેક્ટ કરવાની જરૂર છે:

```typescript
// ઉદાહરણ ડેટા સ્ટ્રક્ચર જે આપણે બનાવવા માંગીએ છીએ
interface ProductData {
  name: string;
  price: number;
  rating: { score: number, count: number };
  features: string[];
  status: string; // In Stock, Out of Stock, etc.
}

interface ResultData {
  categories: {
    electronics: {
      phones: ProductData[];
      laptops: ProductData[];
      accessories: ProductData[];
    };
    clothing: {
      mens: {
        shirts: ProductData[];
        pants: ProductData[];
      };
      womens: {
        dresses: ProductData[];
        tops: ProductData[];
      };
    };
  };
  featured_products: FeaturedProduct[];
}
```

### Crawlee સાથે મુખ્ય ક્રોલિંગ ખ્યાલો

1. **રિક્વેસ્ટ કતાર વ્યવસ્થાપન**

Crawlee આપોઆપ કતાર હેન્ડલ કરે છે, પરંતુ અહીં આપણે તેને કેવી રીતે કૉન્ફિગર કરીએ છીએ:

```typescript
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    // દરેક રિક્વેસ્ટ હેન્ડલ કરે છે
    async requestHandler({ $, request, enqueueLinks }) {
        // પેજને પ્રોસેસ કરો
        const data = extractPageData($);

        // પેજ પર મળેલા નવા URLs આપોઆપ કતારમાં મૂકો
        await enqueueLinks({
            selector: 'a',
            baseUrl: request.loadedUrl,
        });
    },
    // સમાંતર રિક્વેસ્ટ્સને મર્યાદિત કરો
    maxConcurrency: 10,
});
```

2. **URL હેન્ડલિંગ**

Crawlee બિલ્ટ-ઇન URL હેન્ડલિંગ અને નોર્મલાઇઝેશન પ્રદાન કરે છે:

```typescript
await crawler.run([startUrl]);

// અથવા વધુ કૉન્ફિગરેશન સાથે:
await crawler.addRequests([{
    url: startUrl,
    userData: {
        label: 'start',
    },
}]);
```

3. **રાઉટ હેન્ડલિંગ**

વિવિધ URLs ને ચોક્કસ હેન્ડલર્સ પર રાઉટ કરો:

```typescript
const crawler = new CheerioCrawler({
    async requestHandler({ $, request }) {
        const { label } = request.userData;

        switch (label) {
            case 'category':
                return handleCategory($);
            case 'product':
                return handleProduct($);
            default:
                return handleHomepage($);
        }
    },
});
```

4. **ડેટા કલેક્શન**

Crawlee એકત્ર કરેલા ડેટા માટે બિલ્ટ-ઇન સ્ટોરેજ પ્રદાન કરે છે:

```typescript
const crawler = new CheerioCrawler({
    async requestHandler({ $, pushData }) {
        const productData = extractProduct($);
        await pushData(productData);
    },
});
```

### વેબ ક્રોલિંગ બેસ્ટ પ્રેક્ટિસિસ

જ્યારે Crawlee ઘણી નિમ્ન-સ્તરની ચિંતાઓનું સંચાલન કરે છે, તમારે હજુ પણ વિચારણા કરવી જોઈએ:

1. **કૉન્ફિગરેશન**
   - યોગ્ય રેટ લિમિટ્સ સેટ કરો
   - રિટ્રાઇ સ્ટ્રેટેજીઓ કૉન્ફિગર કરો
   - અર્થપૂર્ણ યુઝર-એજન્ટ સ્ટ્રિંગ્સ સેટ કરો

2. **એરર હેન્ડલિંગ**
   - Crawlee ની બિલ્ટ-ઇન એરર હેન્ડલિંગનો ઉપયોગ કરો
   - કસ્ટમ એરર કૉલબેક્સ અમલમાં મૂકો
   - અર્થપૂર્ણ ડાયગ્નોસ્ટિક માહિતી લૉગ કરો

3. **ડેટા ઓર્ગેનાઇઝેશન**
   - તમારા ડેટાને સુસંગત રીતે સ્ટ્રક્ચર કરો
   - રાઉટિંગ માટે રિક્વેસ્ટ લેબલ્સનો ઉપયોગ કરો
   - Crawlee ની ડેટાસેટ સુવિધાઓનો લાભ લો

4. **રિસોર્સ મેનેજમેન્ટ**
   - maxConcurrency યોગ્ય રીતે કૉન્ફિગર કરો
   - જરૂર પડે ત્યારે maxRequestsPerCrawl નો ઉપયોગ કરો
   - મેમરી વપરાશ પર દેખરેખ રાખો

### પડકાર

તમારું કાર્ય એક Crawlee-આધારિત ક્રોલર બનાવવાનું છે જે:

1. હોમપેજથી શરૂ થાય અને બધી પ્રોડક્ટ કેટેગરીઓ શોધે
2. દરેક કેટેગરી અને સબકેટેગરી પેજની મુલાકાત લે
3. દરેક લિસ્ટિંગમાંથી પ્રોડક્ટની માહિતી એક્સ્ટ્રેક્ટ કરે
4. ડેટાને સંરચિત ફોર્મેટમાં ગોઠવે
5. બહુવિધ જગ્યાએ દેખાતા પ્રોડક્ટ્સ હેન્ડલ કરે (જેમ કે ફીચર્ડ અને કેટેગરીમાં)

સાઇટમાં વિવિધ કેટેગરીઓમાં લગભગ 25-30 પ્રોડક્ટ્સ છે, વિવિધ લેઆઉટ અને માહિતીના માળખા સાથે. તમારા ક્રોલરે એક વ્યાપક ડેટાસેટ બનાવવો જોઈએ જે કેટેગરીઓ અને પ્રોડક્ટ્સ વચ્ચેના વંશવેલો સંબંધને જાળવે.

### તમારા સોલ્યુશનનું પરીક્ષણ

આના માટે પરીક્ષણ કરો:
- સંપૂર્ણતા: શું તમને બધા પ્રોડક્ટ્સ મળ્યા?
- ચોકસાઈ: શું એક્સ્ટ્રેક્ટેડ ડેટા સાચો છે?
- સ્ટ્રક્ચર: શું ડેટા યોગ્ય રીતે ગોઠવાયો છે?
- કાર્યક્ષમતા: તમે કેટલી રિક્વેસ્ટ્સ કરી?

`_solved/chapter6/` માં હલ કરેલું ઉદાહરણ Crawlee નો ઉપયોગ કરીને સંદર્ભ અમલીકરણ પ્રદાન કરે છે. કાર્યક્ષમ મલ્ટિ-પેજ ક્રોલિંગ અને ડેટા ઓર્ગેનાઇઝેશન માટે લાઇબ્રેરીની સુવિધાઓનો લાભ કેવી રીતે લેવો તે સમજવા માટે તેનો અભ્યાસ કરો.

ખુશ ક્રોલિંગ!